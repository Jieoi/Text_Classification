{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04aefc8d",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b><h1> Couserwork Assignment: Text Classification</h1></b>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d3c79a",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>University of London</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850a4fc",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>BSc in Computer Science</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3cb96",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>Natural Language Processing</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fbe0bc",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36069138",
   "metadata": {},
   "source": [
    "<h1> Table of Content</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0acbe",
   "metadata": {},
   "source": [
    "<style>\n",
    "ul {\n",
    "    font-size: 20px;\n",
    "}\n",
    "\n",
    "ul ul {\n",
    "    font-size: 15px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <a href=\"#intro\">1. Introduction</a>\n",
    "        <ul>\n",
    "            <li><a href=\"#1.1\">1.1 Problem Area</a></li>\n",
    "            <li><a href=\"#1.2\">1.2 Objectives</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#section2\">Section 2</a>\n",
    "        <ul>\n",
    "            <li><a href=\"#subsection2.1\">Subsection 2.1</a></li>\n",
    "            <li><a href=\"#subsection2.2\">Subsection 2.2</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#section3\">Section 3</a>\n",
    "        <ul>\n",
    "            <li><a href=\"#subsection3.1\">Subsection 3.1</a></li>\n",
    "            <li><a href=\"#subsection3.2\">Subsection 3.2</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f79f63",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291f5f6",
   "metadata": {},
   "source": [
    "<h2 id=\"intro\">1. Introduction</h2>\n",
    "<h3 id=\"1.1\">1.1 Problem area</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a464044f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5846b2d",
   "metadata": {},
   "source": [
    "<h3 id=\"\">Data Extraction</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660ef232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library needed for data analysis\n",
    "import os\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6e8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folders\n",
    "pos_folder = r'C:\\Users\\xjie\\Documents\\SIM\\Y3S2\\nlp\\aclImdb\\train\\pos'\n",
    "neg_folder = r'C:\\Users\\xjie\\Documents\\SIM\\Y3S2\\nlp\\aclImdb\\train\\neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a8a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions;\n",
    "\n",
    "# Count the number of files in the data folder\n",
    "def count_files(folder_path):\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: {folder_path} is not a valid directory.\")\n",
    "        return\n",
    "\n",
    "    file_count = 0\n",
    "\n",
    "    for _, _, files in os.walk(folder_path):\n",
    "        file_count += len(files)\n",
    "\n",
    "    return file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f13f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of files in positive training folder is: 12500\n"
     ]
    }
   ],
   "source": [
    "# Example the positive training data file\n",
    "file_count = count_files(pos_folder)\n",
    "print(f\"The number of files in positive training folder is: {file_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d30de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of files in positive training folder is: 12500\n"
     ]
    }
   ],
   "source": [
    "# Example the positive training data file\n",
    "file_count = count_files(pos_folder)\n",
    "print(f\"The number of files in positive training folder is: {file_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ac5c6",
   "metadata": {},
   "source": [
    "As the training data set is <b>too big</b> for the computer, only <b>2000 data</b> from positive and negative folder will be selected to make the program runs faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69e9fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "# Function to extract unique ID and rating from file name\n",
    "def extract_info(file_name):\n",
    "    id_rating = file_name.split('.')[0]\n",
    "    unique_id, rating = id_rating.split('_')\n",
    "    return int(unique_id), int(rating)\n",
    "\n",
    "# Function to extract the data from the file based on the aclImdb data structure\n",
    "def extract_files(folder, num_files, sentiment):\n",
    "    files = random.sample(os.listdir(folder), num_files)\n",
    "    data = pd.DataFrame(columns=['Sentiment', 'Unique ID', 'Rating', 'Content'])\n",
    "    \n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read() # read content for the review\n",
    "        unique_id, rating = extract_info(file_name)  # get ID and rating from file name\n",
    "        file_data = pd.DataFrame({'Sentiment': [sentiment], 'Unique ID': [unique_id], 'Rating': [rating], 'Content': [content]})\n",
    "        data = pd.concat([data, file_data], ignore_index=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d91458ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>3325</td>\n",
       "      <td>7</td>\n",
       "      <td>OK, I would not normally watch a Farrelly brot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>8233</td>\n",
       "      <td>8</td>\n",
       "      <td>Fido is a story about more well mannered zombi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>7214</td>\n",
       "      <td>7</td>\n",
       "      <td>Ira Levin's Deathtrap is one of those mystery ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>5069</td>\n",
       "      <td>7</td>\n",
       "      <td>The movie is more about Pony than Grey Owl. It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>I was up late flipping cable channels one nigh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment Unique ID Rating   \n",
       "0       pos      3325      7  \\\n",
       "1       pos      8233      8   \n",
       "2       pos      7214      7   \n",
       "3       pos      5069      7   \n",
       "4       pos        69     10   \n",
       "\n",
       "                                             Content  \n",
       "0  OK, I would not normally watch a Farrelly brot...  \n",
       "1  Fido is a story about more well mannered zombi...  \n",
       "2  Ira Levin's Deathtrap is one of those mystery ...  \n",
       "3  The movie is more about Pony than Grey Owl. It...  \n",
       "4  I was up late flipping cable channels one nigh...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting training data\n",
    "pos_data = extract_files(pos_folder, 2000, 'pos')\n",
    "neg_data = extract_files(neg_folder, 2000, 'neg')\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "raw_data = pd.concat([pos_data, neg_data], ignore_index=True)\n",
    "\n",
    "# Display the start of the data\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f51fc5",
   "metadata": {},
   "source": [
    "<h3 id=\"\">Prelimary analysis and data cleaning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4edab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>3683</td>\n",
       "      <td>8</td>\n",
       "      <td>3995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>pos</td>\n",
       "      <td>7480</td>\n",
       "      <td>1</td>\n",
       "      <td>I was one of those \"few Americans\" that grew u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>801</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment  Unique ID  Rating   \n",
       "count       4000       4000    4000  \\\n",
       "unique         2       3683       8   \n",
       "top          pos       7480       1   \n",
       "freq        2000          2     801   \n",
       "\n",
       "                                                  Content  \n",
       "count                                                4000  \n",
       "unique                                               3995  \n",
       "top     I was one of those \"few Americans\" that grew u...  \n",
       "freq                                                    2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examining the data\n",
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac1535",
   "metadata": {},
   "source": [
    "It was noted that both Unique ID and Content has duplicated values. Thus, an algorithm will be implemented to check for these values and handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bafd4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "# Check for repetetion of data\n",
    "def find_duplicates(dataframe, column):\n",
    "    duplicates = dataframe[dataframe[column].duplicated(keep=False)]\n",
    "    duplicates_sorted = duplicates.sort_values(by=column)\n",
    "\n",
    "    if duplicates_sorted.empty:\n",
    "        print(\"No repeated data entry found.\")\n",
    "    else:\n",
    "        print(\"Repeated data entry/entries found in \"+str(column))\n",
    "        duplicates_pairs = duplicates_sorted.groupby(column).apply(lambda x: x.reset_index(drop=True))\n",
    "        duplicates_pairs.reset_index(drop=True, inplace=True)\n",
    "        return duplicates_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f03f7919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated data entry/entries found in Content\n",
      "Repeated data entry/entries found in Unique ID\n",
      "--------------------------------------------------\n",
      "Duplication in content:\n",
      "  Sentiment Unique ID Rating   \n",
      "0       neg     10332      1  \\\n",
      "1       neg       281      1   \n",
      "2       neg      7355      4   \n",
      "3       neg     12216      4   \n",
      "4       neg      2156      1   \n",
      "5       neg     11575      1   \n",
      "6       neg      5984      4   \n",
      "7       neg      5983      4   \n",
      "8       neg      5952      4   \n",
      "9       neg      5550      4   \n",
      "\n",
      "                                             Content  \n",
      "0  Awful, simply awful. It proves my theory about...  \n",
      "1  Awful, simply awful. It proves my theory about...  \n",
      "2  Don't get me wrong - I love David Suchet as Po...  \n",
      "3  Don't get me wrong - I love David Suchet as Po...  \n",
      "4  I have been familiar with the fantastic book o...  \n",
      "5  I have been familiar with the fantastic book o...  \n",
      "6  I was one of those \"few Americans\" that grew u...  \n",
      "7  I was one of those \"few Americans\" that grew u...  \n",
      "8  You do realize that you've been watching the E...  \n",
      "9  You do realize that you've been watching the E...  \n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "Duplication in ID:\n",
      "    Sentiment Unique ID Rating   \n",
      "0         neg        38      2  \\\n",
      "1         pos        38     10   \n",
      "2         neg        50      4   \n",
      "3         pos        50     10   \n",
      "4         neg        53      3   \n",
      "..        ...       ...    ...   \n",
      "629       pos     12367      9   \n",
      "630       pos     12372      7   \n",
      "631       neg     12372      3   \n",
      "632       neg     12390      4   \n",
      "633       pos     12390      8   \n",
      "\n",
      "                                               Content  \n",
      "0    or anyone who was praying for the sight of Al ...  \n",
      "1    One of the very best Three Stooges shorts ever...  \n",
      "2    Three part \"horror\" film with some guy in a bo...  \n",
      "3    My children just happened to stop at this movi...  \n",
      "4    I'm studying Catalan, and was delighted to fin...  \n",
      "..                                                 ...  \n",
      "629  Well, after long anticipation after seeing a f...  \n",
      "630  The Sentinel is a movie that was recommended t...  \n",
      "631  I had high expectations of this movie (the tit...  \n",
      "632  After seeing a heavily censored version of thi...  \n",
      "633  Bizarre horror movie filled with famous faces ...  \n",
      "\n",
      "[634 rows x 4 columns]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "duplicate_content = find_duplicates(raw_data, 'Content')\n",
    "duplicate_ID = find_duplicates(raw_data, 'Unique ID')\n",
    "\n",
    "if duplicate_content is not None:\n",
    "    print('-' * 50)\n",
    "    print(\"Duplication in content:\")\n",
    "    print(duplicate_content)\n",
    "    print('-' * 50)\n",
    "if duplicate_ID is not None:\n",
    "    print('-' * 50)\n",
    "    print(\"Duplication in ID:\")\n",
    "    print(duplicate_ID)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c8773",
   "metadata": {},
   "source": [
    "As a <b>new ID is assigned</b> as the index for the data frame, there is <b>no need</b> for the Unique ID from positive sentiment and negative sentiment folder. Thus, the <b>unique ID column will be dropped</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d158ec5",
   "metadata": {},
   "source": [
    "Moreover, the duplicated content will be <b>remove<b/> and new data will be loaded to replace them so that the dataset remains balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feca68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unique ID column from the data\n",
    "org_ID_removed_data = raw_data.drop(\"Unique ID\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e266cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace duplicates with other values\n",
    "def get_balance_data_with_duplicates(duplicate_content, original_data, pos_folder, neg_folder):\n",
    "    # Remove the first occurrence of unique items in the duplicated content\n",
    "    data_to_remove = duplicate_content.drop_duplicates(subset='Content', keep='first')\n",
    "    org_ID_removed_data = original_data.drop_duplicates(subset='Content', keep='first')\n",
    "\n",
    "    # Count the number of pos and neg entries in 'data_to_remove'\n",
    "    pos_count = data_to_remove[data_to_remove['Sentiment'] == 'pos'].shape[0]\n",
    "    neg_count = data_to_remove[data_to_remove['Sentiment'] == 'neg'].shape[0]\n",
    "\n",
    "    # Add the removed duplicates such that the data is balanced\n",
    "    added_pos_data = extract_files(pos_folder, pos_count, 'pos')\n",
    "    added_neg_data = extract_files(neg_folder, neg_count, 'neg')\n",
    "\n",
    "    # Remove the 'Unique ID' column from added data\n",
    "    added_pos_data.drop(\"Unique ID\", axis=1, inplace=True)\n",
    "    added_neg_data.drop(\"Unique ID\", axis=1, inplace=True)\n",
    "\n",
    "    added_data = pd.concat([added_pos_data, added_neg_data], ignore_index=True)\n",
    "    balanced_data = pd.concat([org_ID_removed_data, added_data], ignore_index=True)\n",
    "\n",
    "    return balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "424ebdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_data_balancing(dataframe, column, pos_folder, neg_folder):\n",
    "    duplicates = find_duplicates(dataframe, column)\n",
    "\n",
    "    if duplicates is None:\n",
    "        print(\"No more duplicates found.\")\n",
    "        return dataframe\n",
    "\n",
    "    # Call get_balance_data_with_duplicates with the duplicate content\n",
    "    balanced_data = get_balance_data_with_duplicates(duplicates, dataframe, pos_folder, neg_folder)\n",
    "\n",
    "    # Recursively check for duplicates\n",
    "    print(\"Recursive call to balance the data.\")\n",
    "    return recursive_data_balancing(balanced_data, column, pos_folder, neg_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab7f46d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated data entry/entries found in Content\n",
      "Recursive call to balance the data.\n",
      "No repeated data entry found.\n",
      "No more duplicates found.\n"
     ]
    }
   ],
   "source": [
    "balanced_data = recursive_data_balancing(org_ID_removed_data, 'Content', pos_folder, neg_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25248cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment    0\n",
       "Rating       0\n",
       "Content      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null data\n",
    "balanced_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c48da5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "pos    2000\n",
       "neg    2000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double checking the total number of data entries\n",
    "balanced_data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb8e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bcfe89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

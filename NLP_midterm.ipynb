{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04aefc8d",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <b><h1> Couserwork Assignment: Text Classification</h1></b>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d3c79a",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>University of London</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850a4fc",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>BSc in Computer Science</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3cb96",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>Natural Language Processing</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fbe0bc",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36069138",
   "metadata": {},
   "source": [
    "<h1> Table of Content</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0acbe",
   "metadata": {},
   "source": [
    "<style>\n",
    "ul {\n",
    "    font-size: 20px;\n",
    "}\n",
    "\n",
    "ul ul {\n",
    "    font-size: 15px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <a href=\"#intro\">1. Introduction</a>\n",
    "        <ul>\n",
    "            <li><a href=\"#1.1\">1.1 Problem Area</a></li>\n",
    "            <li><a href=\"#1.2\">1.2 Objectives</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#section2\">Section 2</a>\n",
    "        <ul>\n",
    "            <li><a href=\"#subsection2.1\">Subsection 2.1</a></li>\n",
    "            <li><a href=\"#subsection2.2\">Subsection 2.2</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#section3\">Section 3</a>\n",
    "        <ul>\n",
    "            <li><a href=\"#subsection3.1\">Subsection 3.1</a></li>\n",
    "            <li><a href=\"#subsection3.2\">Subsection 3.2</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f79f63",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291f5f6",
   "metadata": {},
   "source": [
    "<h2 id=\"intro\">1. Introduction</h2>\n",
    "<h3 id=\"1.1\">1.1 Problem area</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a464044f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5846b2d",
   "metadata": {},
   "source": [
    "<h3 id=\"\">Data Extraction</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660ef232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library needed for data analysis\n",
    "import os\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a6e8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folders\n",
    "pos_folder = r'C:\\Users\\xjie\\Documents\\SIM\\Y3S2\\nlp\\aclImdb\\train\\pos'\n",
    "neg_folder = r'C:\\Users\\xjie\\Documents\\SIM\\Y3S2\\nlp\\aclImdb\\train\\neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a8a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions;\n",
    "\n",
    "# Count the number of files in the data folder\n",
    "def count_files(folder_path):\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: {folder_path} is not a valid directory.\")\n",
    "        return\n",
    "\n",
    "    file_count = 0\n",
    "\n",
    "    for _, _, files in os.walk(folder_path):\n",
    "        file_count += len(files)\n",
    "\n",
    "    return file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f13f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of files in positive training folder is: 12500\n"
     ]
    }
   ],
   "source": [
    "# Example the positive training data file\n",
    "file_count = count_files(pos_folder)\n",
    "print(f\"The number of files in positive training folder is: {file_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d30de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of files in positive training folder is: 12500\n"
     ]
    }
   ],
   "source": [
    "# Example the positive training data file\n",
    "file_count = count_files(pos_folder)\n",
    "print(f\"The number of files in positive training folder is: {file_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ac5c6",
   "metadata": {},
   "source": [
    "As the training data set is <b>too big</b> for the computer, only <b>2000 data</b> from positive and negative folder will be selected to make the program runs faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69e9fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "# Function to extract unique ID and rating from file name\n",
    "def extract_info(file_name):\n",
    "    id_rating = file_name.split('.')[0]\n",
    "    unique_id, rating = id_rating.split('_')\n",
    "    return int(unique_id), int(rating)\n",
    "\n",
    "# Function to extract the data from the file based on the aclImdb data structure\n",
    "def extract_files(folder, num_files, sentiment):\n",
    "    files = random.sample(os.listdir(folder), num_files)\n",
    "    data = pd.DataFrame(columns=['Sentiment', 'Unique ID', 'Rating', 'Content'])\n",
    "    \n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read() # read content for the review\n",
    "        unique_id, rating = extract_info(file_name)  # get ID and rating from file name\n",
    "        file_data = pd.DataFrame({'Sentiment': [sentiment], 'Unique ID': [unique_id], 'Rating': [rating], 'Content': [content]})\n",
    "        data = pd.concat([data, file_data], ignore_index=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91458ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting training data\n",
    "pos_data = extract_files(pos_folder, 2000, 'pos')\n",
    "neg_data = extract_files(neg_folder, 2000, 'neg')\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "raw_data = pd.concat([pos_data, neg_data], ignore_index=True)\n",
    "\n",
    "# Display the start of the data\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f51fc5",
   "metadata": {},
   "source": [
    "<h3 id=\"\">Prelimary analysis and data cleaning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4edab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the data\n",
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e593f41",
   "metadata": {},
   "source": [
    "It was noted that both Unique ID and Content has duplicated values. Thus, an algorithm will be implemented to check for these values and handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bafd4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "# Check for repetetion of data\n",
    "def find_duplicates(dataframe, column):\n",
    "    duplicates = dataframe[dataframe[column].duplicated(keep=False)]\n",
    "    duplicates_sorted = duplicates.sort_values(by=column)\n",
    "\n",
    "    if duplicates_sorted.empty:\n",
    "        print(\"No repeated data entry found.\")\n",
    "    else:\n",
    "        print(\"Repeated data entry/entries found in \"+str(column))\n",
    "        duplicates_pairs = duplicates_sorted.groupby(column).apply(lambda x: x.reset_index(drop=True))\n",
    "        duplicates_pairs.reset_index(drop=True, inplace=True)\n",
    "        return duplicates_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_content = find_duplicates(raw_data, 'Content')\n",
    "duplicate_ID = find_duplicates(raw_data, 'Unique ID')\n",
    "\n",
    "if duplicate_content is not None:\n",
    "    print('-' * 50)\n",
    "    print(\"Duplication in content:\")\n",
    "    print(duplicate_content)\n",
    "    print('-' * 50)\n",
    "if duplicate_ID is not None:\n",
    "    print('-' * 50)\n",
    "    print(\"Duplication in ID:\")\n",
    "    print(duplicate_ID)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c8773",
   "metadata": {},
   "source": [
    "As a <b>new ID is assigned</b> as the index for the data frame, there is <b>no need</b> for the Unique ID from positive sentiment and negative sentiment folder. Thus, the <b>unique ID column will be dropped</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d158ec5",
   "metadata": {},
   "source": [
    "Moreover, the duplicated content will be <b>remove<b/> and new data will be loaded to replace them so that the dataset remains balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de471dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unique ID column from the data\n",
    "org_ID_removed_data = raw_data.drop(\"Unique ID\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e266cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace duplicates with other values\n",
    "def get_balance_data_with_duplicates(duplicate_content, original_data, pos_folder, neg_folder):\n",
    "    # Remove the first occurrence of unique items in the duplicated content\n",
    "    data_to_remove = duplicate_content.drop_duplicates(subset='Content', keep='first')\n",
    "    org_ID_removed_data = original_data.drop_duplicates(subset='Content', keep='first')\n",
    "\n",
    "    # Count the number of pos and neg entries in 'data_to_remove'\n",
    "    pos_count = data_to_remove[data_to_remove['Sentiment'] == 'pos'].shape[0]\n",
    "    neg_count = data_to_remove[data_to_remove['Sentiment'] == 'neg'].shape[0]\n",
    "\n",
    "    # Add the removed duplicates such that the data is balanced\n",
    "    added_pos_data = extract_files(pos_folder, pos_count, 'pos')\n",
    "    added_neg_data = extract_files(neg_folder, neg_count, 'neg')\n",
    "\n",
    "    # Remove the 'Unique ID' column from added data\n",
    "    added_pos_data.drop(\"Unique ID\", axis=1, inplace=True)\n",
    "    added_neg_data.drop(\"Unique ID\", axis=1, inplace=True)\n",
    "\n",
    "    added_data = pd.concat([added_pos_data, added_neg_data], ignore_index=True)\n",
    "    balanced_data = pd.concat([org_ID_removed_data, added_data], ignore_index=True)\n",
    "\n",
    "    return balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_data_balancing(dataframe, column, pos_folder, neg_folder):\n",
    "    duplicates = find_duplicates(dataframe, column)\n",
    "\n",
    "    if duplicates is None:\n",
    "        print(\"No more duplicates found.\")\n",
    "        return dataframe\n",
    "\n",
    "    # Call get_balance_data_with_duplicates with the duplicate content\n",
    "    balanced_data = get_balance_data_with_duplicates(duplicates, dataframe, pos_folder, neg_folder)\n",
    "\n",
    "    # Recursively check for duplicates\n",
    "    print(\"Recursive call to balance the data.\")\n",
    "    return recursive_data_balancing(balanced_data, column, pos_folder, neg_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data = recursive_data_balancing(org_ID_removed_data, 'Content', pos_folder, neg_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25248cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null data\n",
    "balanced_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48da5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double checking the total number of data entries\n",
    "balanced_data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb8e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8980a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
